{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "740px",
        "left": "0px",
        "right": "1628px",
        "top": "161px",
        "width": "253px"
      },
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "building_image_classifier.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": "true",
        "id": "GAvCRC7ofDoj"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Building-an-Image-Classifier-with-Differential-Privacy\" data-toc-modified-id=\"Building-an-Image-Classifier-with-Differential-Privacy-1\">Building an Image Classifier with Differential Privacy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1.1\">Overview</a></span></li><li><span><a href=\"#Hyper-parameters\" data-toc-modified-id=\"Hyper-parameters-1.2\">Hyper-parameters</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.3\">Data</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-1.4\">Model</a></span></li><li><span><a href=\"#Prepare-for-Training\" data-toc-modified-id=\"Prepare-for-Training-1.5\">Prepare for Training</a></span></li><li><span><a href=\"#Train-the-network\" data-toc-modified-id=\"Train-the-network-1.6\">Train the network</a></span></li><li><span><a href=\"#Test-the-network-on-test-data\" data-toc-modified-id=\"Test-the-network-on-test-data-1.7\">Test the network on test data</a></span></li><li><span><a href=\"#Tips-and-Tricks\" data-toc-modified-id=\"Tips-and-Tricks-1.8\">Tips and Tricks</a></span></li><li><span><a href=\"#Private-Model-vs-Non-Private-Model-Performance\" data-toc-modified-id=\"Private-Model-vs-Non-Private-Model-Performance-1.9\">Private Model vs Non-Private Model Performance</a></span></li></ul></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlvlBxSffDop"
      },
      "source": [
        "# Building an Image Classifier with Differential Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1T0ofCffDop"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this tutorial we will learn to do the following:\n",
        "  1. Learn about privacy specific hyper-parameters related to DP-SGD \n",
        "  2. Learn about ModelInspector, incompatible layers, and use model rewriting utility. \n",
        "  3. Train a differentially private ResNet18 for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrzLBvxVfDoq"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ0QAt-6fDoq"
      },
      "source": [
        "To train a model with Opacus there are three privacy-specific hyper-parameters that must be tuned for better performance:\n",
        "\n",
        "* Max Grad Norm: The maximum L2 norm of per-sample gradients before they are aggregated by the averaging step.\n",
        "* Noise Multiplier: The amount of noise sampled and added to the average of the gradients in a batch.\n",
        "* Delta: The target δ of the (ϵ,δ)-differential privacy guarantee. Generally, it should be set to be less than the inverse of the size of the training dataset. In this tutorial, it is set to $10^{−5}$ as the CIFAR10 dataset has 50,000 training points.\n",
        "\n",
        "We use the hyper-parameter values below to obtain results in the last section:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMwiZfhlfDor"
      },
      "source": [
        "MAX_GRAD_NORM = 1.2\n",
        "NOISE_MULTIPLIER = .38\n",
        "DELTA = 1e-5\n",
        "\n",
        "LR = 1e-3\n",
        "NUM_WORKERS = 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMwwU85_fDor"
      },
      "source": [
        "There's another constraint we should be mindful of&mdash;memory. To balance peak memory requirement, which is proportional to `batch_size^2`, and training performance, we use virtual batches. With virtual batches we can separate physical steps (gradient computation) and logical steps (noise addition and parameter updates): use larger batches for training, while keeping memory footprint low. Below we will specify two constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jULKcn3ufDos"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "VIRTUAL_BATCH_SIZE = 512\n",
        "assert VIRTUAL_BATCH_SIZE % BATCH_SIZE == 0 # VIRTUAL_BATCH_SIZE should be divisible by BATCH_SIZE\n",
        "N_ACCUMULATION_STEPS = int(VIRTUAL_BATCH_SIZE / BATCH_SIZE)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZNdiTXlfDos"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l97U7Xg1fDos"
      },
      "source": [
        "Now, let's load the CIFAR10 dataset. We don't use data augmentation here because, in our experiments, we found that data augmentation lowers utility when training with DP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuAfwQGdfDot"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# These values, specific to the CIFAR10 dataset, are assumed to be known.\n",
        "# If necessary, they can be computed with modest privacy budget.\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD_DEV = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD_DEV),\n",
        "])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVHAnkgQfDot"
      },
      "source": [
        "Using torchvision datasets, we can load CIFAR10 and transform the PILImage images to Tensors of normalized range [-1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdFT6pCzfDot",
        "outputId": "a86cebea-e55c-4401-9405-629e25314a5a"
      },
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "\n",
        "DATA_ROOT = '../cifar10'\n",
        "\n",
        "train_dataset = CIFAR10(\n",
        "    root=DATA_ROOT, train=True, download=True, transform=transform)\n",
        "\n",
        "SAMPLE_RATE = BATCH_SIZE / len(train_dataset)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    batch_sampler=UniformWithReplacementSampler(\n",
        "        num_samples=len(train_dataset),\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "    ),\n",
        ")\n",
        "\n",
        "test_dataset = CIFAR10(\n",
        "    root=DATA_ROOT, train=False, download=True, transform=transform)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_IBBiAmfDou"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-WCt9UAfDou"
      },
      "source": [
        "from torchvision import models\n",
        "\n",
        "model = models.resnet18(num_classes=10)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC6Ns36bfDou"
      },
      "source": [
        "Now, let’s check if the model is compatible with Opacus. Opacus does not support all type of Pytorch layers. To check if your model is compatible with the privacy engine, we have provided a util class to validate your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAmp4PZ7fDou"
      },
      "source": [
        "If you run these commands, you will get the following error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "kQjDqbNcfDou",
        "outputId": "cdc0204c-f851-4966-dace-b7b9aeb36cd9"
      },
      "source": [
        "from opacus.dp_model_inspector import DPModelInspector\n",
        "\n",
        "inspector = DPModelInspector()\n",
        "inspector.validate(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IncompatibleModuleException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIncompatibleModuleException\u001b[0m               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-162c21ea82b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minspector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDPModelInspector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minspector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opacus/dp_model_inspector.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minspector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviolators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n{inspector.message}: {inspector.violators}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompatibleModuleException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIncompatibleModuleException\u001b[0m: Model contains incompatible modules.\nSome modules are not valid.: ['Main.bn1', 'Main.layer1.0.bn1', 'Main.layer1.0.bn2', 'Main.layer1.1.bn1', 'Main.layer1.1.bn2', 'Main.layer2.0.bn1', 'Main.layer2.0.bn2', 'Main.layer2.0.downsample.1', 'Main.layer2.1.bn1', 'Main.layer2.1.bn2', 'Main.layer3.0.bn1', 'Main.layer3.0.bn2', 'Main.layer3.0.downsample.1', 'Main.layer3.1.bn1', 'Main.layer3.1.bn2', 'Main.layer4.0.bn1', 'Main.layer4.0.bn2', 'Main.layer4.0.downsample.1', 'Main.layer4.1.bn1', 'Main.layer4.1.bn2']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT1qrwWrfDov"
      },
      "source": [
        "Let us modify the model to work with Opacus. From the output above, you can see that the BatchNorm layers are not supported because they compute the mean and variance across the batch, creating a dependency between samples in a batch, a privacy violation. One way to modify our model is to replace all the BatchNorm layers with [GroupNorm](https://arxiv.org/pdf/1803.08494.pdf) using the `convert_batchnorm_modules` util function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWRdDZUZfDow",
        "outputId": "d24a6ab1-d279-47a5-87d6-300b406d647d"
      },
      "source": [
        "from opacus.utils import module_modification\n",
        "\n",
        "model = module_modification.convert_batchnorm_modules(model)\n",
        "inspector = DPModelInspector()\n",
        "print(f\"Is the model valid? {inspector.validate(model)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is the model valid? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3hrsSCTfDow"
      },
      "source": [
        "For maximal speed, we can check if CUDA is available and supported by the PyTorch installation. If GPU is available, set the `device` variable to your CUDA-compatible device. We can then transfer the neural network onto that device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2C46dh8fDow"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDzKWB6ZfDox"
      },
      "source": [
        "We then define our optimizer and loss function. Opacus’ privacy engine can attach to any (first-order) optimizer.  You can use your favorite&mdash;Adam, Adagrad, RMSprop&mdash;as long as it has an implementation derived from [torch.optim.Optimizer](https://pytorch.org/docs/stable/optim.html). In this tutorial, we're going to use [RMSprop](https://pytorch.org/docs/stable/optim.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxwwsYiMfDox"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=LR)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znroC0oDfDox"
      },
      "source": [
        "## Prepare for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXEqy4zffDox"
      },
      "source": [
        "We will define a util function to calculate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_TNHJg5fDox"
      },
      "source": [
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCApBDeKfDox"
      },
      "source": [
        "We now attach the privacy engine initialized with the privacy hyperparameters defined earlier. There’s also the enigmatic-looking parameter `alphas`, which we won’t touch for the time being."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYfd7WZ1fDoy",
        "outputId": "91926265-a45c-41cc-f2ae-74cc2c83afe3"
      },
      "source": [
        "from opacus import PrivacyEngine\n",
        "\n",
        "print(f\"Using sigma={NOISE_MULTIPLIER} and C={MAX_GRAD_NORM}\")\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    model,\n",
        "    sample_rate=SAMPLE_RATE * N_ACCUMULATION_STEPS,\n",
        "    alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
        "    noise_multiplier=NOISE_MULTIPLIER,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "privacy_engine.attach(optimizer)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using sigma=0.38 and C=1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtM0RJm8fDoy"
      },
      "source": [
        "We will then define our train function. This function will train the model for one epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyXzWXIXfDoy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def train(model, train_loader, optimizer, epoch, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    for i, (images, target) in enumerate(train_loader):        \n",
        "        images = images.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # compute output\n",
        "        output = model(images)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "        labels = target.detach().cpu().numpy()\n",
        "        \n",
        "        # measure accuracy and record loss\n",
        "        acc = accuracy(preds, labels)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        top1_acc.append(acc)\n",
        "        \n",
        "        loss.backward()\n",
        "        \t\n",
        "        # take a real optimizer step after N_VIRTUAL_STEP steps t\n",
        "        if ((i + 1) % N_ACCUMULATION_STEPS == 0) or ((i + 1) == len(train_loader)):\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            optimizer.virtual_step() # take a virtual step\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(DELTA)\n",
        "            print(\n",
        "                f\"\\tTrain Epoch: {epoch} \\t\"\n",
        "                f\"Loss: {np.mean(losses):.6f} \"\n",
        "                f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
        "                f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n",
        "            )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L2Cu3g2fDoz"
      },
      "source": [
        "Next, we will define our test function to validate our model on our test dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IBWasQCfDoz"
      },
      "source": [
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, target in test_loader:\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "            labels = target.detach().cpu().numpy()\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "    top1_avg = np.mean(top1_acc)\n",
        "\n",
        "    print(\n",
        "        f\"\\tTest set:\"\n",
        "        f\"Loss: {np.mean(losses):.6f} \"\n",
        "        f\"Acc: {top1_avg * 100:.6f} \"\n",
        "    )\n",
        "    return np.mean(top1_acc)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWYgzFVofDo0"
      },
      "source": [
        "## Train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWUmoF2-fDo0",
        "outputId": "ea06f9ff-a5cb-4380-e97d-8109c83532f8"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(20), desc=\"Epoch\", unit=\"epoch\"):\n",
        "    train(model, train_loader, optimizer, epoch + 1, device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/20 [00:00<?, ?epoch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 1 \tLoss: 2.493264 Acc@1: 12.096774 (ε = 0.10, δ = 1e-05)\n",
            "\tTrain Epoch: 1 \tLoss: 2.761356 Acc@1: 14.456262 (ε = 14.58, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   5%|▌         | 1/20 [01:05<20:38, 65.21s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 2 \tLoss: 1.955629 Acc@1: 25.641026 (ε = 17.11, δ = 1e-05)\n",
            "\tTrain Epoch: 2 \tLoss: 1.767638 Acc@1: 37.946047 (ε = 19.29, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|█         | 2/20 [02:10<19:32, 65.11s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 3 \tLoss: 1.648330 Acc@1: 46.835443 (ε = 20.79, δ = 1e-05)\n",
            "\tTrain Epoch: 3 \tLoss: 1.723294 Acc@1: 46.021707 (ε = 22.31, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  15%|█▌        | 3/20 [03:15<18:28, 65.22s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 4 \tLoss: 1.556890 Acc@1: 50.769231 (ε = 23.78, δ = 1e-05)\n",
            "\tTrain Epoch: 4 \tLoss: 1.730266 Acc@1: 48.891415 (ε = 25.08, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 4/20 [04:20<17:22, 65.18s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 5 \tLoss: 1.592981 Acc@1: 53.968254 (ε = 26.15, δ = 1e-05)\n",
            "\tTrain Epoch: 5 \tLoss: 1.692724 Acc@1: 51.628847 (ε = 27.26, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 5/20 [05:26<16:20, 65.35s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 6 \tLoss: 1.790667 Acc@1: 50.000000 (ε = 28.33, δ = 1e-05)\n",
            "\tTrain Epoch: 6 \tLoss: 1.691323 Acc@1: 52.989614 (ε = 29.45, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|███       | 6/20 [06:31<15:15, 65.41s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 7 \tLoss: 1.557785 Acc@1: 54.014599 (ε = 30.52, δ = 1e-05)\n",
            "\tTrain Epoch: 7 \tLoss: 1.674198 Acc@1: 54.527030 (ε = 31.63, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  35%|███▌      | 7/20 [07:36<14:07, 65.17s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 8 \tLoss: 1.935706 Acc@1: 50.000000 (ε = 32.61, δ = 1e-05)\n",
            "\tTrain Epoch: 8 \tLoss: 1.673220 Acc@1: 56.032113 (ε = 33.45, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 8/20 [08:41<13:00, 65.06s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 9 \tLoss: 1.574659 Acc@1: 52.830189 (ε = 34.26, δ = 1e-05)\n",
            "\tTrain Epoch: 9 \tLoss: 1.657451 Acc@1: 57.054291 (ε = 35.10, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  45%|████▌     | 9/20 [09:46<11:56, 65.16s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 10 \tLoss: 1.855417 Acc@1: 57.037037 (ε = 35.90, δ = 1e-05)\n",
            "\tTrain Epoch: 10 \tLoss: 1.649551 Acc@1: 58.181273 (ε = 36.74, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 10/20 [10:51<10:49, 64.98s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 11 \tLoss: 1.901847 Acc@1: 58.823529 (ε = 37.54, δ = 1e-05)\n",
            "\tTrain Epoch: 11 \tLoss: 1.637535 Acc@1: 58.539230 (ε = 38.38, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  55%|█████▌    | 11/20 [11:56<09:44, 65.00s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 12 \tLoss: 1.689841 Acc@1: 56.349206 (ε = 39.19, δ = 1e-05)\n",
            "\tTrain Epoch: 12 \tLoss: 1.627441 Acc@1: 59.299434 (ε = 40.02, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 12/20 [13:01<08:40, 65.10s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 13 \tLoss: 1.931030 Acc@1: 57.983193 (ε = 40.83, δ = 1e-05)\n",
            "\tTrain Epoch: 13 \tLoss: 1.631124 Acc@1: 59.830398 (ε = 41.67, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  65%|██████▌   | 13/20 [14:06<07:36, 65.15s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 14 \tLoss: 1.670289 Acc@1: 58.741259 (ε = 42.47, δ = 1e-05)\n",
            "\tTrain Epoch: 14 \tLoss: 1.601755 Acc@1: 60.857855 (ε = 43.31, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  70%|███████   | 14/20 [15:11<06:30, 65.07s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 15 \tLoss: 1.663538 Acc@1: 61.206897 (ε = 44.11, δ = 1e-05)\n",
            "\tTrain Epoch: 15 \tLoss: 1.578390 Acc@1: 61.596206 (ε = 44.95, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 15/20 [16:16<05:24, 64.91s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 16 \tLoss: 1.691741 Acc@1: 61.946903 (ε = 45.72, δ = 1e-05)\n",
            "\tTrain Epoch: 16 \tLoss: 1.591795 Acc@1: 61.336092 (ε = 46.37, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 16/20 [17:21<04:19, 64.91s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 17 \tLoss: 1.885407 Acc@1: 55.384615 (ε = 46.99, δ = 1e-05)\n",
            "\tTrain Epoch: 17 \tLoss: 1.612365 Acc@1: 61.411545 (ε = 47.64, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  85%|████████▌ | 17/20 [18:26<03:15, 65.04s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 18 \tLoss: 1.843095 Acc@1: 61.744966 (ε = 48.26, δ = 1e-05)\n",
            "\tTrain Epoch: 18 \tLoss: 1.617456 Acc@1: 61.773505 (ε = 48.91, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  90%|█████████ | 18/20 [19:31<02:10, 65.12s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 19 \tLoss: 1.120436 Acc@1: 69.354839 (ε = 49.53, δ = 1e-05)\n",
            "\tTrain Epoch: 19 \tLoss: 1.571179 Acc@1: 62.916583 (ε = 50.18, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  95%|█████████▌| 19/20 [20:36<01:04, 64.96s/epoch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 20 \tLoss: 1.641913 Acc@1: 62.015504 (ε = 50.80, δ = 1e-05)\n",
            "\tTrain Epoch: 20 \tLoss: 1.562555 Acc@1: 62.999958 (ε = 51.45, δ = 1e-05)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 20/20 [21:41<00:00, 65.07s/epoch]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3TodEb4fDo0"
      },
      "source": [
        "## Test the network on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMTMkwHufDo0",
        "outputId": "26b82d78-0e21-4d91-a69a-979a3a00db43"
      },
      "source": [
        "top1_acc = test(model, test_loader, device)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTest set:Loss: 1.783089 Acc: 59.612342 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIMXrvuGfDo1"
      },
      "source": [
        "## Tips and Tricks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaMLKtFwfDo1"
      },
      "source": [
        "1. Generally speaking, differentially private training is enough of a regularizer by itself. Adding any more regularization (such as dropouts or data augmentation) is unnecessary and typically hurts performance.\n",
        "2. Tuning MAX_GRAD_NORM is very important. Start with a low noise multiplier like .1, this should give comparable performance to a non-private model. Then do a grid search for the optimal MAX_GRAD_NORM value. The grid can be in the range [.1, 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwJN23oQfDo1"
      },
      "source": [
        "## Private Model vs Non-Private Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRPQuIFnfDo1"
      },
      "source": [
        "Now let us compare how our private model compares with the non-private ResNet18.\n",
        "\n",
        "We trained a non-private ResNet18 model for 20 epochs using the same hyper-parameters as above and with BatchNorm replaced with GroupNorm. The results of that training and the training that is discussed in this tutorial are summarized in the table below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7yMa0rZfDo1"
      },
      "source": [
        "| Model          | Top 1 Accuracy (%) |  ϵ |\n",
        "|----------------|--------------------|---|\n",
        "| ResNet         | 76                 | ∞ |\n",
        "| Private ResNet |         59.61         |  51.45  |"
      ]
    }
  ]
}